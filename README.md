# Dignity Is All You Need

This project investigates the impact of prompt sentiment on Large Language Model (LLM) outputs. By analyzing 20 prompts, each modified to convey negative, neutral, or positive sentiments, the study evaluates the resulting outputs using both VADER sentiment analysis and self-evaluation metrics from the LLMs. 

Please note - this is weekend work, and needs much, much more testing on multitude of prompts, llms, and languages - information there in is based on evidence sourced from one model, and a single pass across 20 prompts! more to come

## Table of Contents

- [Project Overview](#project-overview)
- [Directory Structure](#directory-structure)
- [Getting Started](#getting-started)
  - [Prerequisites](#prerequisites)
  - [Installation](#installation)
  - [Running the Analysis](#running-the-analysis)
- [Data Files](#data-files)
- [Logging](#logging)
- [Contributing](#contributing)
- [License](#license)
- [Contact](#contact)

## Project Overview

The primary objective of this project is to assess how varying sentiments in input prompts influence the responses generated by LLMs. The analysis employs both quantitative methods, such as VADER sentiment scores, and qualitative assessments derived from the LLMs' self-evaluation capabilities.

Prompt Tone Affects Evaluated Quality:

When we used a GPT‑4–based evaluator to score the responses on a 1–5 quality scale (with criteria such as factual correctness, clarity, coherence, thoroughness, and overall helpfulness), the outputs generated from negatively phrased prompts received a higher average quality grade (mean ≈ 4.75, median = 5, mode = 5) compared to those from neutral or positive prompts (both averaging around 4.10 with median and mode equal to 4).
Conclusion: Contrary to our initial assumption that impolite prompts might degrade quality, our evidence shows that, at least in our evaluator’s judgment, negative prompts tend to elicit responses that are rated higher on quality.
Variability in Output Consistency:

Although the negative prompts scored higher on average in terms of quality, our earlier automated measures (such as those based on VADER combined with readability and lexical diversity metrics) indicated that outputs from negative prompts exhibited greater variability (a higher standard deviation) compared to those from neutral or positive prompts.
Conclusion: Negative prompts result in less consistent performance. In some cases, they may trigger exceptionally high-quality responses, but the variability is much greater—suggesting that the quality of outputs from impolite prompts is less stable.
Multiple Evaluation Perspectives Yield Different Insights:

Our experiments used two complementary approaches: (a) automated metrics (VADER-based sentiment, readability, and lexical diversity) and (b) a human-like GPT‑4 evaluator. The automated metrics suggested that negative outputs had higher variability, while the GPT‑4 evaluator assigned them higher average quality scores.
Conclusion: The method of evaluation matters. The GPT‑4 evaluator seems to reward the detailed or direct nature of responses to negative prompts, while automated metrics highlight inconsistency. This divergence underscores the importance of using multiple evaluation approaches when assessing LLM output quality.
Implications for Prompt Engineering:

Our data suggest that while negatively phrased prompts might sometimes coax out more comprehensive or detailed answers (reflected in the higher average quality grade), they also carry the risk of generating outputs with less consistency.
Conclusion: When designing prompts for LLMs, one must consider the trade-off between potentially higher-quality but more variable responses (from impolite prompts) and the more consistent (but not necessarily higher-rated) outputs from neutral or positive prompts. This has implications for applications where consistency is critical.
In summary, our work today provides concrete evidence that:

Negative prompt tones can yield outputs that are rated higher in quality (by our GPT‑4 evaluator) than those generated from neutral or positive prompts.
However, the same negative prompts produce a wider range of output quality, indicating lower consistency.
The evaluation method significantly influences the interpretation of “quality,” so a combination of automated and LLM-based assessments is advisable for a fuller picture.

### Key Limitations

Evaluator Dependency and Subjectivity:
Our quality grading relies on GPT‑4 as an evaluator using a pre‐defined prompt. While this approach leverages GPT‑4’s language capabilities, its assessments may be influenced by its internal biases or may not fully capture all dimensions of “quality” (e.g., factual correctness or domain-specific expertise). Human evaluations or multiple independent evaluators could provide a more robust benchmark.

Narrow Quality Metrics:
The automated metrics (e.g., VADER sentiment, readability, and lexical diversity) offer only a limited view of quality. They do not account for correctness, depth of explanation, or contextual relevance—factors that might be crucial for some applications.

Sample Size and Data Diversity:
Our experiments are based on a specific set of core prompts and responses, which  limit the generalizability of our findings. The results could be affected by the particular tasks, domains, or even the language used in our prompts.

API and Cost Constraints:
Our work is subject to practical limitations, such as API rate limits and usage quotas, which restrict the number of samples we can evaluate. These constraints may have forced us to work with a smaller dataset, impacting the statistical robustness of our conclusions.

Interplay of Multiple Evaluation Methods:
We employed both automated measures and GPT‑4’s evaluation, which sometimes yielded differing insights (e.g., higher average quality scores for negative prompts by GPT‑4, but increased variability in automated measures). Reconciling these differences remains a challenge and suggests that a multi-dimensional evaluation framework is necessary.


## Directory Structure

The repository is organized as follows:

```
dignity_is_all_you_need/
├── analysis.py
├── config.py
├── graded_llm_responses.csv
├── graded_llm_responses_with_quality.csv
├── graded_quality_responses.csv
├── llm_based_quality_eval.py
├── logs/
│   └── app.log
├── main.py
├── prompts.py
├── requirements.txt
├── results/
│   └── llm_responses_results.csv
└── utils.py
```

- **analysis.py**: Contains functions and methods for processing and analyzing LLM responses.
- **config.py**: Holds configuration settings, such as API keys and model parameters.
- **graded_llm_responses.csv**: Dataset of LLM responses with assigned sentiment grades.
- **graded_llm_responses_with_quality.csv**: Extended dataset including quality metrics for each response.
- **graded_quality_responses.csv**: Focuses on the quality assessments of LLM responses.
- **llm_based_quality_eval.py**: Script dedicated to evaluating the quality of LLM outputs.
- **logs/app.log**: Log file capturing runtime information and errors.
- **main.py**: The main script to initiate the analysis workflow.
- **prompts.py**: Contains the set of prompts used in the study, categorized by sentiment.
- **requirements.txt**: Lists all Python dependencies required for the project.
- **results/llm_responses_results.csv**: Compiled results from the analysis, including sentiment scores and evaluations.
- **utils.py**: Utility functions supporting various tasks across the project.

## Getting Started

### Prerequisites

- **Python 3.8 or higher**: Ensure Python is installed on your system. You can download it from the [official Python website](https://www.python.org/downloads/).

### Installation

1. **Clone the Repository**:
   ```bash
   git clone https://github.com/SvetimFM/dignity_is_all_you_need.git
   cd dignity_is_all_you_need
   ```

2. **Set Up a Virtual Environment**:
   ```bash
   python3 -m venv venv
   source venv/bin/activate  # On Windows, use venv\Scripts\activate
   ```

3. **Install Dependencies**:
   ```bash
   pip install --upgrade pip
   pip install -r requirements.txt
   ```

### Running the Analysis

To execute the main analysis pipeline:

```bash
python main.py
```

This script will process the prompts, generate LLM responses, and perform sentiment and quality evaluations.

## Data Files

- **graded_llm_responses.csv**: Contains LLM responses with corresponding sentiment grades.
- **graded_llm_responses_with_quality.csv**: Extends the previous file by adding quality metrics for each response.
- **graded_quality_responses.csv**: Focuses solely on the quality assessments of the responses.

These files are generated during the analysis and can be found in the root directory.

## Logging

Runtime information, including errors and process flow details, is logged in `logs/app.log`. Review this file for debugging and understanding the analysis workflow.

## Contributing

Thank you to authors of [Should We Respect LLMs? A Cross-Lingual Study on
the Influence of Prompt Politeness on LLM Performance](https://arxiv.org/pdf/2402.14531) 
Ziqi Yin, Hao Wang, Kaito Horio, Daisuke Kawahara, Satoshi Sekine - of Waseda University

We welcome contributions to enhance the project. To contribute:

1. **Fork the Repository**: Click the "Fork" button at the top right of the repository page.
2. **Create a New Branch**:
   ```bash
   git checkout -b feature/YourFeatureName
   ```
3. **Make Your Changes**: Implement your feature or fix.
4. **Commit Your Changes**:
   ```bash
   git commit -m "Add Your Feature"
   ```
5. **Push to Your Fork**:
   ```bash
   git push origin feature/YourFeatureName
   ```
6. **Open a Pull Request**: Navigate to the original repository and open a pull request with a descriptive title and explanation.

Please ensure your code adheres to the project's coding standards and includes relevant tests.

## License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.

## Contact

For questions or suggestions, please open an issue in this repository.
```
